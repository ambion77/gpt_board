import express from 'express';
import cron from 'node-cron';
import axios from 'axios';
import * as cheerio from 'cheerio';
import db from "../db.js"; // MySQL ì—°ê²° íŒŒì¼
import loadQueries from "../queryLoader.js"; // XML ê¸°ë°˜ ì¿¼ë¦¬ ë¡œë”

const router = express.Router();
let queries = {};

// ğŸ”¹ XML ì¿¼ë¦¬ ë¡œë“œ (ë¹„ë™ê¸°)
(async () => {
  queries = await loadQueries();
})();

// í¬ë¡¤ë§ í•¨ìˆ˜
async function crawlQuotes() {
    try {
        console.log('=== Starting daily news crawling job ===');
        const { data } = await axios.get("https://www.yna.co.kr/rss/news.xml");
        const $ = cheerio.load(data, { xmlMode: true });
        const news = [];

        // RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ì•„ì´í…œ íŒŒì‹±
        $('item').slice(0, 10).each((_, el) => {
            const $el = $(el);
            news.push({
                title: $el.find('title').text().replace(/\<\!\[CDATA\[(.*?)\]\]\>/g, '$1').trim(),
                link: $el.find('link').text().trim(),
                pub_date: new Date($el.find('pubDate').text()),
                crawled_date: new Date()
            });
        });

        // ëª¨ë“  ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ìˆœíšŒí•˜ë©´ì„œ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        for (const item of news) {
            try {
                console.log(`News title saved: "${item.title}"`);
                console.log(`News link saved: "${item.link}"`);
                console.log(`News pub_date saved: "${item.pub_date}"`);
                console.log(`News crawled_date saved: "${item.crawled_date}"`);
                await db.execute(queries.insertNews, [
                    item.title,
                    item.link,
                    item.pub_date,
                    item.crawled_date
                ]);
            } catch (err) {
                console.error('Error saving news:', err);
                continue;
            }
        }

        console.log(`=== Successfully saved quotes to database ===`);
        return news;
    } catch (error) {
        console.error('Crawling job failed:', error);
        throw error;
    }
}

// ë§¤ì¼ ìì •ì— ì‹¤í–‰ë˜ëŠ” ìŠ¤ì¼€ì¤„ëŸ¬
const scheduleCrawling = () => {
    console.log('=== Crawling scheduler initialized ===');
    // ë§¤ì¼ ìì •(0 0 * * *)ì— ì‹¤í–‰
    // ë§¤ì‹œê°(0 * * * *)ì— ì‹¤í–‰
    cron.schedule('0 14 * * *', crawlQuotes);
};

// í¬ë¡¤ë§ ìˆ˜ë™ ì‹¤í–‰ API
router.get("/crawling", async (req, res) => {
    try {
        const news = await crawlQuotes(); // crawlQuotesë¥¼ í˜¸ì¶œ
        res.json({ 
            message: 'í¬ë¡¤ë§ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.',
            newsCount: news.length
        });
    } catch (error) {
        console.error('í¬ë¡¤ë§ ì‹¤í–‰ ì‹¤íŒ¨:', error);
        res.status(500).json({ error: 'í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.' });
    }
});

export default router;  // default export ì¶”ê°€. server.jsì—ì„œ crawlingScheduleRoutes ì´ë¦„ìœ¼ë¡œ import ê°€ëŠ¥
export { scheduleCrawling };    // server.jsì—ì„œ scheduleCrawling í•¨ìˆ˜ë¥¼ ë‚´ë³´ë‚´ê¸°